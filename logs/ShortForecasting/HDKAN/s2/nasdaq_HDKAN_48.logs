Args in experiment:
Namespace(random_seed=2025, enable_visual=False, save_model=False, percent=100, patch_len=36, degree1=7, degree2=5, is_training=1, model_id='nasdaq_36_48', model='HDKAN', data='custom', root_path='D:\\work\\datasets\\all_datasets\\short-term', data_path='nasdaq.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=36, label_len=0, pred_len=48, use_revin=1, enc_in=12, dec_in=7, c_out=7, d_model=128, dropout=0.0, alpha=0.35, embed='timeF', num_workers=0, itr=1, train_epochs=20, batch_size=16, patience=5, learning_rate=2.5e-05, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1', inverse=False)
Use GPU: cuda:0
>>>>>>>start training : nasdaq_36_48_HDKAN_custom_ftM_bz16_sl36_pl48_lr2.5e-05_dm128_d17_d25_patl36_dr0.0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 2656
val 346
test 735
	iters: 100, epoch: 1 | loss: 0.5184479
	speed: 0.0192s/iter; left time: 61.9586s
Epoch: 1 cost time: 1.8438494205474854
Epoch: 1, Steps: 166 | Train Loss: 0.5782544 Vali Loss: 0.6377348 Test Loss: 0.2826088
Validation loss decreased (inf --> 0.637735).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 2 | loss: 0.5153690
	speed: 0.0186s/iter; left time: 56.8943s
Epoch: 2 cost time: 1.4760746955871582
Epoch: 2, Steps: 166 | Train Loss: 0.5665373 Vali Loss: 0.5830333 Test Loss: 0.2387820
Validation loss decreased (0.637735 --> 0.583033).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 3 | loss: 0.5322026
	speed: 0.0127s/iter; left time: 36.8143s
Epoch: 3 cost time: 1.0440609455108643
Epoch: 3, Steps: 166 | Train Loss: 0.5363024 Vali Loss: 0.5375524 Test Loss: 0.2067833
Validation loss decreased (0.583033 --> 0.537552).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.6117547
	speed: 0.0177s/iter; left time: 48.3239s
Epoch: 4 cost time: 2.374656915664673
Epoch: 4, Steps: 166 | Train Loss: 0.5258927 Vali Loss: 0.5250726 Test Loss: 0.1989120
Validation loss decreased (0.537552 --> 0.525073).  Saving model ...
Updating learning rate to 2e-05
	iters: 100, epoch: 5 | loss: 0.5302558
	speed: 0.0346s/iter; left time: 88.5742s
Epoch: 5 cost time: 2.9728333950042725
Epoch: 5, Steps: 166 | Train Loss: 0.5212552 Vali Loss: 0.5187489 Test Loss: 0.1961897
Validation loss decreased (0.525073 --> 0.518749).  Saving model ...
Updating learning rate to 1.6000000000000003e-05
	iters: 100, epoch: 6 | loss: 0.6441283
	speed: 0.0347s/iter; left time: 83.0673s
Epoch: 6 cost time: 2.9606661796569824
Epoch: 6, Steps: 166 | Train Loss: 0.5187943 Vali Loss: 0.5114353 Test Loss: 0.1948639
Validation loss decreased (0.518749 --> 0.511435).  Saving model ...
Updating learning rate to 1.2800000000000003e-05
	iters: 100, epoch: 7 | loss: 0.4654962
	speed: 0.0344s/iter; left time: 76.5415s
Epoch: 7 cost time: 2.9801151752471924
Epoch: 7, Steps: 166 | Train Loss: 0.5171518 Vali Loss: 0.5118940 Test Loss: 0.1953306
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.0240000000000002e-05
	iters: 100, epoch: 8 | loss: 0.4969408
	speed: 0.0347s/iter; left time: 71.4914s
Epoch: 8 cost time: 2.9626519680023193
Epoch: 8, Steps: 166 | Train Loss: 0.5158346 Vali Loss: 0.5132392 Test Loss: 0.1953805
EarlyStopping counter: 2 out of 5
Updating learning rate to 8.192000000000002e-06
	iters: 100, epoch: 9 | loss: 0.6433825
	speed: 0.0345s/iter; left time: 65.3193s
Epoch: 9 cost time: 2.973179817199707
Epoch: 9, Steps: 166 | Train Loss: 0.5148775 Vali Loss: 0.5106027 Test Loss: 0.1952509
Validation loss decreased (0.511435 --> 0.510603).  Saving model ...
Updating learning rate to 6.553600000000003e-06
	iters: 100, epoch: 10 | loss: 0.5575778
	speed: 0.0348s/iter; left time: 60.0907s
Epoch: 10 cost time: 2.9875710010528564
Epoch: 10, Steps: 166 | Train Loss: 0.5141859 Vali Loss: 0.5094242 Test Loss: 0.1949937
Validation loss decreased (0.510603 --> 0.509424).  Saving model ...
Updating learning rate to 5.242880000000002e-06
	iters: 100, epoch: 11 | loss: 0.5549817
	speed: 0.0321s/iter; left time: 50.1365s
Epoch: 11 cost time: 2.1254422664642334
Epoch: 11, Steps: 166 | Train Loss: 0.5136305 Vali Loss: 0.5097720 Test Loss: 0.1958501
EarlyStopping counter: 1 out of 5
Updating learning rate to 4.1943040000000025e-06
	iters: 100, epoch: 12 | loss: 0.4632593
	speed: 0.0173s/iter; left time: 24.1506s
Epoch: 12 cost time: 1.5125041007995605
Epoch: 12, Steps: 166 | Train Loss: 0.5131918 Vali Loss: 0.5087266 Test Loss: 0.1958436
Validation loss decreased (0.509424 --> 0.508727).  Saving model ...
Updating learning rate to 3.355443200000002e-06
	iters: 100, epoch: 13 | loss: 0.4614186
	speed: 0.0184s/iter; left time: 22.6554s
Epoch: 13 cost time: 1.5916903018951416
Epoch: 13, Steps: 166 | Train Loss: 0.5128602 Vali Loss: 0.5089902 Test Loss: 0.1959859
EarlyStopping counter: 1 out of 5
Updating learning rate to 2.6843545600000017e-06
	iters: 100, epoch: 14 | loss: 0.4708273
	speed: 0.0178s/iter; left time: 18.9278s
Epoch: 14 cost time: 1.3191449642181396
Epoch: 14, Steps: 166 | Train Loss: 0.5126117 Vali Loss: 0.5076211 Test Loss: 0.1961519
Validation loss decreased (0.508727 --> 0.507621).  Saving model ...
Updating learning rate to 2.1474836480000014e-06
	iters: 100, epoch: 15 | loss: 0.5176041
	speed: 0.0118s/iter; left time: 10.5852s
Epoch: 15 cost time: 1.0485026836395264
Epoch: 15, Steps: 166 | Train Loss: 0.5124202 Vali Loss: 0.5073938 Test Loss: 0.1962864
Validation loss decreased (0.507621 --> 0.507394).  Saving model ...
Updating learning rate to 1.717986918400001e-06
	iters: 100, epoch: 16 | loss: 0.5331591
	speed: 0.0272s/iter; left time: 19.8703s
Epoch: 16 cost time: 2.9484658241271973
Epoch: 16, Steps: 166 | Train Loss: 0.5122633 Vali Loss: 0.5066664 Test Loss: 0.1963317
Validation loss decreased (0.507394 --> 0.506666).  Saving model ...
Updating learning rate to 1.374389534720001e-06
	iters: 100, epoch: 17 | loss: 0.4564214
	speed: 0.0344s/iter; left time: 19.4082s
Epoch: 17 cost time: 2.957676887512207
Epoch: 17, Steps: 166 | Train Loss: 0.5121408 Vali Loss: 0.5073024 Test Loss: 0.1964086
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.099511627776001e-06
	iters: 100, epoch: 18 | loss: 0.6076210
	speed: 0.0345s/iter; left time: 13.7579s
Epoch: 18 cost time: 3.147035837173462
Epoch: 18, Steps: 166 | Train Loss: 0.5120479 Vali Loss: 0.5079357 Test Loss: 0.1965274
EarlyStopping counter: 2 out of 5
Updating learning rate to 8.796093022208008e-07
	iters: 100, epoch: 19 | loss: 0.4513875
	speed: 0.0364s/iter; left time: 8.4844s
Epoch: 19 cost time: 3.0303502082824707
Epoch: 19, Steps: 166 | Train Loss: 0.5119708 Vali Loss: 0.5073142 Test Loss: 0.1965616
EarlyStopping counter: 3 out of 5
Updating learning rate to 7.036874417766406e-07
	iters: 100, epoch: 20 | loss: 0.4720501
	speed: 0.0356s/iter; left time: 2.3854s
Epoch: 20 cost time: 3.0285961627960205
Epoch: 20, Steps: 166 | Train Loss: 0.5119101 Vali Loss: 0.5072388 Test Loss: 0.1966157
EarlyStopping counter: 4 out of 5
Updating learning rate to 5.629499534213126e-07
>>>>>>>testing : nasdaq_36_48_HDKAN_custom_ftM_bz16_sl36_pl48_lr2.5e-05_dm128_d17_d25_patl36_dr0.0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 735
test shape: (735, 48, 12) (735, 48, 12)
test shape: (735, 48, 12) (735, 48, 12)
mse:0.19633165001869202, mae:0.290120929479599
